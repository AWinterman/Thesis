\chapter{ Introduction }
\section{Test of Equivalence in Context}

 Tests of equivalence are a class of hypothesis tests designed to assess whether two data sets are drawn from populations whose probability distributions do not substantively differ. They are not a new statistical idea. Most commonly used by manufacturers of pharmaceuticals, tests of equivalence have found widespread use in the industry since the early 1980’s, following a 1979 Food and Drug Admistration decision stating that new, generic drugs would be approved if they could be shown to be ``bioequivalent" to existing, approved drugs \cite{wellek}. Once a drug has exhausted the duration of its patent, it can be reproduced by any company, often at a fraction of the price of the patented version. These reproductions are called ``generic" drugs. As a matter of course, both the generic and patented original version of the drug are nominally chemically identical, but they might differ in a number of ways, including milling procedure, the chemicals used in the delivery system, etc. Two drugs are bioequivalent if their action upon the body is sufficiently similar \cite{Birkett}, as demonstrated by clinical trials employing tests of equivalence. Because of the billions of dollars involved in the generic drug industry, there has been substantial theoretical attention, and many biopharmaceutical applications thoroughly described (for an exposition of such applications, see Wellek \cite{wellek}).

The purpose of this thesis is to describe two such tests of equivalence, and extend their implementation to the context of DNA microarray experiments, a scientific tool in the field of genetics. In this application, tests of equivalence have not been in common use - a Web of Science search (Summer 2010) has uncovered only two papers addressing the subject: \cite{QiuandCui} and \cite{eijgelaar} - but are necessary to the corroboration of common and desirable claims.

\section{The Two Color DNA Microarray }

Microarrays are small glass slides imprinted with thousands of fragments of DNA. Many copies of each fragment are placed in high density on a clearly delineated spot, which is referred to interchangeably as a feature or a probe. Ideally, the DNA in each probe contains the complete code for a gene of interest to the scientist and, as such, the DNA fragment is called a gene, although this may be inaccurate.\footnote{A gene is defined as a segment of DNA which codes for the production of a particular protein. Apart from the technical difficulty of slicing up DNA at precise points, it can also be difficult to tell exactly where one gene starts and another ends. The features on the array may or may not contain the full genetic code for a given gene. See \cite{Wirth:2000} for an algorithm which attempts to solve this problem.} Microarrays are designed to compare the amounts of gene-coding fragments of DNA or RNA found in different tissues through a phenomenon known as DNA hybridization. Essentially, hybridization refers to a process in which single strands of complementary DNA or RNA chemically bind to one another. Non-complementary strands sometimes bind together as well, but a better fit induces more fragments to bind. A two color microarray experiment can be divided into two steps. First the complement of the DNA (or RNA) which encodes the genes of interest to the scientist is imprinted on the microarray itself. Then, DNA is taken from two different sources, dyed two colors, and then washed over the prepared microarrays. The dyed DNA is induced to hybridize, i.e. bind to the DNA in the probes on the array. Then, lasers are used to assess relative intensities of fluorescence of the two dyes coloring the features on the array. 

For example, if we were comparing two phenotypes\footnote{A phenotype is an observable trait of an organism, such as morphology or behavior. In this context the the word means a group of organisms exhibiting the observable trait.} of a lemur species, say lemur phenotype $A$ and lemur phenotype $B$, we would prepare the slide with fragments of DNA which we suspect to be present in both phenotypes. Then we would extract DNA from $A$ and $B$ and dye the DNA red (Cyanine 5 dye) and green (Cyanine 3 dye) respectively\footnote{Actually, Cyanine 5 is simply a little more reddish and Cyanine 3 is a little more greenish. The laser used to assess intensity is, in any case, precisely calibrated to the frequency of light each dye reflects.}. We would wash our solutions of DNA and dye over the microarrays, and induce the DNA to hybridize. Finally we would shine a green laser and a red laser at the features on the microarray slide. The intensity of the reflected red or green light from each feature is proportional to the amount of DNA of that color bound to the feature. Hence the ratio of reflected light from a given feature is the ratio of the amount of DNA (which matches the DNA in the feature) from each lemur in the solutions washed over the slide  \cite{microarray}. If done properly, the ratios of reflected light is linearly related to the relative amount of the given DNA fragment in each of our lemurs. 

Each feature on a microarray slide produces a single data point in a single experiment. Hence if a slide has six thousand features, running it will produce a single data point in each of six thousand experiments (unless of course, fragments of DNA are repeated on each slide). The experimenter must use quite a few slides to obtain a suitable number of data points in each experiment. As microarrays can be expensive, non-commercial microarray experiments often involve many relatively small data sets. They remain, however, computationally intensive - often requiring the analysis of several thousand experiments at once - meaning development of effective tools for analysis of these data sets requires some mathematical training. 

The statistical tools commonly employed in the analysis of microarray data are hypothesis tests of difference, such as the two sided $t$-test, which assumes there is no difference between the two data sets and looks for contradictory evidence. If no evidence of difference is found - that is, if a significance level $\alpha$ test finds that the given gene is \emph{not} differentially expressed - researchers are often tempted to conclude that the gene is equivalently expressed across the tissues under examination at level $\alpha$. However, the assertion that genes not significantly different are significantly equivalent is a logical fallacy born of a seductive misunderstanding of hypothesis testing. A hypothesis test requires the specification of a null hypothesis - that is, a set of non-contradictory assumptions - against which we hope to find evidence. In the microarray context, the null hypothesis might be that there is no difference in gene expression levels across the two groups. We can then either reject or fail to reject this null hypothesis. If we fail to reject the null, we do not have sufficient reason to conclude that the null is true. We have no evidence to the contrary, but we also do not necessarily have any evidence in support\footnote{It is possible to design a test which, when the probability of type II error is sufficiently low, can find evidence for the null. However, in general this is a reckless approach, and logically incorrect besides.} \cite{altman}. Hence a test of difference, which takes as its null hypothesis the assumption of no difference, does not necessarily supply evidence of equivalence.

In the absence of a fully developed test of equivalence, researches are left with ad hoc methods to determine equivalence, which often involve the logical error described in the preceding paragraph. For example, in Adjaye et al. (2004) \cite{adjaye}, the authors describe techniques for assessing Bovine gene expression using Microarrays prepared with stretches of human DNA. The goal of their paper is to demonstrate that certain fragments of Human and Bovine DNA bind equally well to Microarrays prepared with human DNA. After running three different statistical tests to assess significance of differential expression, Adjaye et al. assert, ``thus, we conclude that the level of expression of the individual 349 genes under investigation within human and bovine brain [sic] is roughly the same."  Price et al (2008) commit a similar error in search of genes whose expression during the aging of the Wallﬂower (Erysimum linifolium) \cite{price} remains unchanged between leaves and flowers. They apply a Student's $t$ test of difference and find, ``for 263 probes derived from the leaf cDNA library, 52\% showed up regulated expression with age in leaves, although larger numbers of leaf-derived probes on the array were stable in expression with leaf senescence." Finally, Rodriguez-Lannety et al (2007) \cite{rodriguez} provide guidance as to how one might select housekeeping genes - endogenous experimental controls whose expression remains unchanged across sources - for normalization of microarray experiments in the field of coral and cnidarian biology. The authors ``tested whether the Cy5/Cy3 ratios were not significantly different to ratio = 1 (null hypothesis) using a one sample $t$-test ... filtering out those genes whose ratios were significantly different from one ($p < 0.05$)." That is, they applied a test of difference as a filter, and asserted that those genes not significantly different are genes which might be equivalently expressed\footnote{This careful proposition is correct, for the appropriate test of equivalence}. They go on to apply several other methods to ascertain which genes have similar expression levels. 

All the authors cited above would like to apply a test of equivalence. Without this tool it is difficult to correctly compute the p-values for their results, which hinders the comparison of results from multiple experiments. As both equivalent and differential expression of genes are of interest, tests of equivalence should be a part of any microarray analyst's statistical tool kit. This thesis contains a careful exposition of two different methods for carrying out a test of equivalence, including explicit application to microarray data analysis.

\section{Two Tests in Broad Brush}

Let us suppose we are examining gene expression between the lemur phenotypes $A$ and $B$ as above. A statistician asked to assess whether or not $A$ and $B$ are equivalent would first choose a parameter, $\Delta$, which measures the difference between the two groups, and for that parameter an estimator, $D$, whose value, should gene expression in $A$ be the same as gene expression in $B$, is known. She calls the value of $\Delta$ given no difference $\zeta$. She would employ two real numbers, $\epsilon_1 < \zeta$ and $\epsilon_2 > \zeta$  to specify how similar gene expression in the two species must be to one another to be considered \emph{equivalent} for her purposes. In other words, if it is true that: $$ \Delta \in ( \epsilon_1 , \epsilon_2), $$ then we say $A$ and $B$ are equivalent. A test of equivalence assesses whether or not the difference as measured by $\Delta$ is substantive -- two samples are declared equivalent if and only if the difference between them (as estimated by $D$) is so small so as to not matter.\footnote{It is still left to the scientist to determine what constitutes a difference that matters.} 
 
To this end, one might compute $ D $ and an associated confidence interval, $I$, of appropriate level; rejecting the hypothesis of non-equivalence should that confidence interval lie completely inside some critical interval, i.e. $(\epsilon_1, \epsilon_2)$. 

In Chapter II, a two one sided Student's $t$-test  (a parametric test)  and a bootstrap test (a non-parametric test) are described. The t-test assumes normally distributed measurement error, an assumption not necessarily well founded. The bootstrap relies on fewer assumptions but is more computationally intensive. In Chapter III the implementation of both tests in the microarray context is described. In Chapter IV the power of both tests is assessed using artificial data sets. Chapter V concludes by suggesting future areas of research.
